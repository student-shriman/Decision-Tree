{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Maths behind Decision Tree Classifier\n",
    "\n",
    "Before we see the python implementation of decision tree. let's first understand the math behind the decision tree classfication. We will see how all the above mentioned terms are used for splitting.\n",
    "\n",
    "We will use a simple dataset which contains information about students of different classes and gender and see whether they stay in school's hostel or not.\n",
    "\n",
    "<img src='images/data_class.PNG' width=\"320\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's try and understand how the root node is selected by calcualting gini impurity. We will use the above mentioned data.\n",
    "\n",
    "We have two features which we can use for nodes: \"Class\" and \"Gender\".\n",
    "We will calculate gini impurity for each of the features and then select that feature which has least gini impurity.\n",
    "\n",
    "Let's review the formula for calculating ginni impurity:\n",
    "\n",
    "<img src='images/gini.PNG' width=\"200\">\n",
    "\n",
    "Let's start with class, we will try to gini impurity for all different values in \"class\". \n",
    "\n",
    "<img src='images/1.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/2.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/3.1.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/3.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/4.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/5.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/6.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/7.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/8.PNG' width=\"500\">\n",
    "\n",
    "This is how our Decision tree node is selected by calculating gini impurity for each node individually.\n",
    "If the number of feautures increases, then we just need to repeat the same steps after the selection of the root node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We will try and find the root nodes for the same dataset by calculating entropy and information gain.\n",
    "\n",
    "DataSet:\n",
    "\n",
    "<img src='images/data_class.PNG' width=\"200\">\n",
    "\n",
    "We have two features and we will try to choose the root node by calculating the information gain by splitting each feature.\n",
    "\n",
    "Let' review the formula for entropy and information gain:\n",
    "\n",
    "<img src='images/formula_entropy.PNG' width=\"300\">\n",
    "\n",
    "<img src='images/inform_gain.PNG' width=\"300\">\n",
    "\n",
    "\n",
    "Let's start with feature \"class\" :\n",
    "\n",
    "<img src='images/9.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/10.1.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/11.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/12.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/13.PNG' width=\"500\">\n",
    "\n",
    "\n",
    "Let' see the information gain from feature \"gender\" :\n",
    "\n",
    "<img src='images/10.2.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/14.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/15.PNG' width=\"500\">\n",
    "\n",
    "<img src='images/16.PNG' width=\"500\">\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Ginni Impurity\n",
    "According to wikipedia, ‘Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset.’\n",
    "It is calculated by multiplying the probability that a given observation is classified into the correct class and sum of all the probabilities when that particular observation is classified into the wrong class.\n",
    "Let’s suppose there are k number of classes and an observation belongs to the class ‘i’, then Ginni impurity is given as:\n",
    "\n",
    "<img src=\"images/ginni.PNG\" width=\"300\">\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Information Gain\n",
    "Information gain calculates the decrease in entropy after splitting a node. It is the difference between entropies before and after the split. The more the information gain, the more entropy is removed. \n",
    "\n",
    "<img src=\"images/info_gain.PNG\" width=\"300\">\n",
    "\n",
    "                                 \n",
    "Where, T is the parent node before split and X is the split node from T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
